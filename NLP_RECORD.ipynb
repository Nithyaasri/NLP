{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGcbDFG15Qs/REIFpyfuPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithyaasri/NLP/blob/main/NLP_RECORD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKENIZATION**"
      ],
      "metadata": {
        "id": "Ss2JlXKgnTU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "text = \"Hello, how are you?\"\n",
        "tokens = encoding.encode(text)\n",
        "decoded_text = encoding.decode(tokens)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Decoded Text:\", decoded_text)\n",
        "print(\"Number of Tokens:\", len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wDoHcERnbyg",
        "outputId": "dc86030d-2b4d-4a79-b55b-0623a17e5e49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello, how are you?\n",
            "Tokens: [9906, 11, 1268, 527, 499, 30]\n",
            "Decoded Text: Hello, how are you?\n",
            "Number of Tokens: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NORMALIZATION**"
      ],
      "metadata": {
        "id": "BOEvcf1ynihY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Cats are running faster than the cat. Dogs were barking loudly.\"\n",
        "normalized = [re.sub(r'[^a-zA-Z]', '', word.lower()) for word in text.split()]\n",
        "print(\"Normalization:\", normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOYbAd92num3",
        "outputId": "7d016c0e-d510-44f3-d084-10c63b519e83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization: ['cats', 'are', 'running', 'faster', 'than', 'the', 'cat', 'dogs', 'were', 'barking', 'loudly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEMMING**"
      ],
      "metadata": {
        "id": "ldjIgFWIoZLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['cats', 'running', 'faster', 'barking']\n",
        "stems = [stemmer.stem(w) for w in words]\n",
        "print(\"Stemming:\", stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovJ1RAmUodSX",
        "outputId": "7bed7087-bade-46e7-9d50-b916f5dc2436"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming: ['cat', 'run', 'faster', 'bark']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LEMMATIZATION**"
      ],
      "metadata": {
        "id": "dA7oh8Z5ohH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = ['cats', 'running', 'better', 'barking']\n",
        "lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
        "print(\"Lemmatization:\", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAq5MQxyolgp",
        "outputId": "948a1440-0432-44a4-bfa5-44c9b77725d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization: ['cat', 'running', 'better', 'barking']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MORPHOLOGY**"
      ],
      "metadata": {
        "id": "eb48wh7Hor-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['running', 'dogs', 'better', 'easily']\n",
        "\n",
        "print(\"Word     | Stem     | Lemma\")\n",
        "for w in words:\n",
        "    print(f\"{w:9} | {stemmer.stem(w):9} | {lemmatizer.lemmatize(w):9}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6tKKg-moz7W",
        "outputId": "5a0e1361-7c58-41f6-c2ed-6e604078534c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word     | Stem     | Lemma\n",
            "running   | run       | running  \n",
            "dogs      | dog       | dog      \n",
            "better    | better    | better   \n",
            "easily    | easili    | easily   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-GRAMS (Unigram, Bigram, Trigram)**"
      ],
      "metadata": {
        "id": "952QwQEVo3jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "\n",
        "text = \"Dogs bark loudly at night\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "bigrams  = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(unigrams)\n",
        "print(bigrams)\n",
        "print(trigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFzQrzfBo_7C",
        "outputId": "5730be1f-a1f4-47b3-cf4d-b972300ab0c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('dogs',), ('bark',), ('loudly',), ('at',), ('night',)]\n",
            "[('dogs', 'bark'), ('bark', 'loudly'), ('loudly', 'at'), ('at', 'night')]\n",
            "[('dogs', 'bark', 'loudly'), ('bark', 'loudly', 'at'), ('loudly', 'at', 'night')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-GRAM SMOOTHING (Laplace / Add-One Smoothing)**"
      ],
      "metadata": {
        "id": "6LcdOQLtp3vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_smooth(text, n=2):\n",
        "    words = text.split()\n",
        "    N = len(words) - n + 1\n",
        "    ngrams = [tuple(words[i:i+n]) for i in range(N)]\n",
        "    V = len(set(ngrams))\n",
        "    for ng in sorted(set(ngrams)):\n",
        "        count = ngrams.count(ng)\n",
        "        print(ng, \"->\", round((count+1)/(N+V), 4))\n",
        "\n",
        "text = \"I love NLP and I love Python\"\n",
        "ngram_smooth(text, 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejWIKay8qIEl",
        "outputId": "41dc360d-7808-42e0-9d4b-3c0f11050e3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I', 'love') -> 0.2727\n",
            "('NLP', 'and') -> 0.1818\n",
            "('and', 'I') -> 0.1818\n",
            "('love', 'NLP') -> 0.1818\n",
            "('love', 'Python') -> 0.1818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART OF SPEECH TAGGING (POS)**"
      ],
      "metadata": {
        "id": "7FokkN6uqam9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input sentence\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print word and its POS tag\n",
        "for token in doc:\n",
        "    print(token.text, \"→\", token.pos_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1CrUTmLqqQh",
        "outputId": "5dcf693b-2843-427e-d0d3-afe753115063"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The → DET\n",
            "quick → ADJ\n",
            "brown → ADJ\n",
            "fox → NOUN\n",
            "jumps → VERB\n",
            "over → ADP\n",
            "the → DET\n",
            "lazy → ADJ\n",
            "dog → NOUN\n",
            ". → PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HIDDEN MARKEV MODEL**"
      ],
      "metadata": {
        "id": "VkPK5qXfq4yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from nltk.tag import hmm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "train = [[('The','DT'),('dog','NN'),('barks','VBZ')],\n",
        "         [('A','DT'),('cat','NN'),('meows','VBZ')]]\n",
        "\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train)\n",
        "print(tagger.tag(['A','dog','meows']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W13dOOyGq_Bt",
        "outputId": "314d5d23-5831-4bef-8cde-4f4ab09c7224"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'DT'), ('dog', 'NN'), ('meows', 'VBZ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BENDING POST TAGGING**"
      ],
      "metadata": {
        "id": "Y8PYwIZwrODI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tag import brill, brill_trainer\n",
        "\n",
        "# Training data\n",
        "data = [[('The','DT'),('dog','NN'),('barks','VBZ')],\n",
        "        [('A','DT'),('cat','NN'),('meows','VBZ')]]\n",
        "\n",
        "# Base tagger\n",
        "base = nltk.UnigramTagger(data)\n",
        "\n",
        "# Train Brill tagger\n",
        "tagger = brill_trainer.BrillTaggerTrainer(base, brill.fntbl37()).train(data)\n",
        "\n",
        "# Test\n",
        "print(tagger.tag(['The','cat','barks']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKvTeYrUrTyY",
        "outputId": "372a4dcf-fe55-4cfc-c3cf-c2cab3b71211"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('cat', 'NN'), ('barks', 'VBZ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SPELLING CORRECTION**"
      ],
      "metadata": {
        "id": "tjTA0lFyrcBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Sample text with spelling errors\n",
        "text = \"I havv goood speling.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Correct the spelling\n",
        "corrected_text = blob.correct()\n",
        "\n",
        "# Output the corrected text\n",
        "print(\"Original:\", text)\n",
        "print(\"Corrected:\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J52QCRpNrg-8",
        "outputId": "4a169fa5-88e6-4dc1-9f14-463222b08a23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I havv goood speling.\n",
            "Corrected: I have good spelling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEDUCTION**"
      ],
      "metadata": {
        "id": "Zd4IbSlVrjBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def stem(text):\n",
        "    return [ps.stem(w) for w in word_tokenize(text.lower())]\n",
        "\n",
        "def deduce(p, h):\n",
        "    return \"entailment\" if h in p else \"no entailment\"\n",
        "\n",
        "print(\"Stems:\", stem(\"running runner runs easily fairer\"))\n",
        "print(\"Deduction:\", deduce(\"All men are mortal Socrates is a man\", \"Socrates is mortal\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St2VJnv4roAE",
        "outputId": "75855bf4-1781-4022-915f-2f980e11f0f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stems: ['run', 'runner', 'run', 'easili', 'fairer']\n",
            "Deduction: no entailment\n"
          ]
        }
      ]
    }
  ]
}